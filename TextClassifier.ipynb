{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5C6Rrd2BDA9",
        "outputId": "1fb78835-e67e-4102-a268-e12a213f38cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import the following\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import copy\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the datasets if dataset not already unzipped\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "r = [1,2,4]\n",
        "for i in r:\n",
        "  file_name = '/content/enron{}_train.zip'.format(i)\n",
        "  with ZipFile(file_name, 'r') as z:\n",
        "    z.extractall()\n",
        "    print('Done')\n",
        "\n",
        "for i in r:\n",
        "  file_name = '/content/enron{}_test.zip'.format(i)\n",
        "  with ZipFile(file_name, 'r') as z:\n",
        "    z.extractall()\n",
        "    print('Done')\n",
        "\n",
        "# there is a abornmality for enron2 dataset path, hence resolving that\n",
        "if os.path.exists('/content/enron2/') == False:\n",
        "  os.mkdir('/content/enron2/')\n",
        "\n",
        "shutil.move(\"/content/train/\", \"/content/enron2/\")\n",
        "shutil.move(\"/content/test/\",\"/content/enron2/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "huZ5qKyiBPxO",
        "outputId": "8184c35f-34ca-4e0e-a1d7-80c124943ec1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/enron2/test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the files/data/emails from their respective paths\n",
        "\n",
        "def get_data(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number\n",
        "  and returns the list of email or files\n",
        "  '''\n",
        "  files_ham = []\n",
        "  files_spam = []\n",
        "  test_files_ham = []\n",
        "  test_files_spam = []\n",
        "  path = os.getcwd()\n",
        "  path_train = os.path.join(path, 'enron{}/train'.format(dataset_num))\n",
        "  path_test = os.path.join(path, 'enron{}/test'.format(dataset_num))\n",
        "  path_train_ham = os.path.join(path_train, 'ham')\n",
        "  path_train_spam = os.path.join(path_train, 'spam')\n",
        "  path_test_ham = os.path.join(path_test, 'ham')\n",
        "  path_test_spam = os.path.join(path_test, 'spam')\n",
        "\n",
        "  # list of paths\n",
        "  files_list_ham = glob.glob(path_train_ham + '/' + '*.txt')\n",
        "  files_list_spam = glob.glob(path_train_spam + '/' + '*txt')\n",
        "  test_files_list_ham = glob.glob(path_test_ham + '/' + '*txt')\n",
        "  test_files_list_spam = glob.glob(path_test_spam + '/' + '*txt')\n",
        "\n",
        "  # Saving the files in a list\n",
        "  for ham in files_list_ham:\n",
        "    files_ham.append(open(ham, 'r', errors='ignore').read())\n",
        "  for spam in files_list_spam:\n",
        "    files_spam.append(open(spam, 'r', errors='replace').read()) \n",
        "\n",
        "  for ham in test_files_list_ham:\n",
        "    test_files_ham.append(open(ham, 'r', errors='ignore').read())\n",
        "  for spam in test_files_list_spam:\n",
        "    test_files_spam.append(open(spam, 'r', errors='replace').read())\n",
        "\n",
        "  size_of_ham = len(files_list_ham)\n",
        "  size_of_spam = len(files_list_spam)\n",
        "  size_of_total = len(files_list_ham) + len(files_list_spam)\n",
        "\n",
        "  return files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam"
      ],
      "metadata": {
        "id": "vRwGXPtpLlMp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bow(temp_ham, temp_spam, Voc_dict):\n",
        "  '''\n",
        "  This function takes the list of files in ham and spam, and also the vocabulary dictionary \n",
        "  and returns the bag of words representation for the dataset\n",
        "  '''\n",
        "  spam_bow = []\n",
        "  ham_bow = []\n",
        "\n",
        "  for file in temp_ham:\n",
        "    temp_dict = copy.deepcopy(Voc_dict)\n",
        "    file1 = re.findall('[a-zA-Z]+', file)\n",
        "    for word in file1:\n",
        "      word = word\n",
        "      if word in Voc_dict:\n",
        "        temp_dict[word] = temp_dict[word] + 1\n",
        "    ham_bow.append(temp_dict)\n",
        "  for file in temp_spam:\n",
        "    temp_dict = copy.deepcopy(Voc_dict)\n",
        "    file1 = re.findall('[a=zA-Z]+', file)\n",
        "    for word in file1:\n",
        "      word = word\n",
        "      if word in Voc_dict:\n",
        "        temp_dict[word] = temp_dict[word] + 1\n",
        "    spam_bow.append(temp_dict)\n",
        "\n",
        "  return ham_bow, spam_bow  \n",
        "\n",
        "def getting_bow(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam):\n",
        "  '''\n",
        "  This function takes the list of files in ham and spam\n",
        "  and returns the bag of words representation for the dataset\n",
        "  '''\n",
        "  word_list = []\n",
        "  ham_train_bow = []\n",
        "  spam_train_bow = []\n",
        "  ham_test_bow = []\n",
        "  spam_test_bow = []\n",
        "\n",
        "  # creating a Vocabulary\n",
        "  for file in files_ham:\n",
        "    file1 = re.findall(\"[a-zA-Z]+\", file)\n",
        "    for word in file1:\n",
        "      word_list.append(word)\n",
        "  for file in files_spam:\n",
        "    file1 = re.findall(\"[a-zA-Z]+\", file)\n",
        "    for word in file1:\n",
        "      word_list.append(word)\n",
        " \n",
        "  Voc = list(set(word_list))\n",
        "\n",
        "  # creating vocabulary dictionary\n",
        "  Voc_dict = {}\n",
        "  for word in Voc:\n",
        "    Voc_dict[word] = 0 \n",
        "\n",
        "  # converting data into Bag of Words\n",
        "  ham_train_bow, spam_train_bow = convert_bow(files_ham, files_spam, Voc_dict)\n",
        "  ham_test_bow, spam_test_bow = convert_bow(test_files_ham, test_files_spam, Voc_dict) \n",
        "\n",
        "  # converting BOW training data into DataFrame to np array\n",
        "  ham_arr = pd.DataFrame(ham_train_bow).to_numpy()\n",
        "  spam_arr = pd.DataFrame(spam_train_bow).to_numpy()\n",
        "\n",
        "  # converting BOW testing data into DataFrame to np array\n",
        "  test_ham_bow_arr = pd.DataFrame(ham_test_bow).to_numpy()\n",
        "  test_spam_bow_arr = pd.DataFrame(spam_test_bow).to_numpy()\n",
        "\n",
        "  train_bow_arr = np.append(ham_arr, spam_arr, axis=0)\n",
        "  test_bow_arr = np.append(test_ham_bow_arr, test_spam_bow_arr, axis=0)\n",
        "\n",
        "  return ham_arr, spam_arr, train_bow_arr, test_bow_arr, Voc_dict"
      ],
      "metadata": {
        "id": "35KdygFtP7LG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bern(temp_ham, temp_spam, Voc_dict):\n",
        "  '''\n",
        "  This function takes the list of files in ham and spam, and also the vocabulary dictionary \n",
        "  and returns the bernoulli representation for the dataset\n",
        "  '''\n",
        "  ham_bern = []\n",
        "  spam_bern = []\n",
        "\n",
        "  for file in temp_ham:\n",
        "    temp_dict = copy.deepcopy(Voc_dict)\n",
        "    file1 = re.findall('[a-zA-Z]+', file)\n",
        "    tr = 0\n",
        "    for word in file1:\n",
        "      word = word\n",
        "      if word in Voc_dict:\n",
        "        temp_dict[word] = 1\n",
        "    ham_bern.append(temp_dict)  \n",
        "  for file in temp_spam:\n",
        "    temp_dict = copy.deepcopy(Voc_dict)\n",
        "    file1 = re.findall('[a=zA-Z]+', file)\n",
        "    for word in file1:\n",
        "      word = word\n",
        "      if word in Voc_dict:\n",
        "        temp_dict[word] = 1\n",
        "    spam_bern.append(temp_dict)\n",
        "\n",
        "  return ham_bern, spam_bern  \n",
        "\n",
        "def getting_bern(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam):\n",
        "  '''\n",
        "  This function takes the list of files in ham and spam \n",
        "  and returns the bernoulli representation for the dataset\n",
        "  '''\n",
        "  word_list = []\n",
        "  ham_train_bern = []\n",
        "  spam_train_bern = []\n",
        "  ham_test_bern = []\n",
        "  spam_test_bern = []\n",
        "\n",
        "  # creating a Vocabulary\n",
        "  for file in files_ham:\n",
        "    file1 = re.findall(\"[a-zA-Z]+\", file)\n",
        "    for word in file1:\n",
        "      word_list.append(word)\n",
        "  for file in files_spam:\n",
        "    file1 = re.findall(\"[a-zA-Z]+\", file)\n",
        "    for word in file1:\n",
        "      word_list.append(word)\n",
        "\n",
        "  Voc = list(set(word_list))\n",
        "\n",
        "  # creating vocabulary dictionary\n",
        "  Voc_dict = {}\n",
        "  for word in Voc:\n",
        "    Voc_dict[word] = 0 \n",
        "\n",
        "  # converting data into Bernoulli model\n",
        "  ham_train_bern, spam_train_bern = convert_bern(files_ham, files_spam, Voc_dict)\n",
        "  ham_test_bern, spam_test_bern = convert_bern(test_files_ham, test_files_spam, Voc_dict) \n",
        "\n",
        "  # converting Bernoulli training data into DataFrame to np array\n",
        "  ham_arr = pd.DataFrame(ham_train_bern).to_numpy()\n",
        "  spam_arr = pd.DataFrame(spam_train_bern).to_numpy()\n",
        "\n",
        "  # converting Bern testing data into DataFrame to np array\n",
        "  test_ham_bern_arr = pd.DataFrame(ham_test_bern).to_numpy()\n",
        "  test_spam_bern_arr = pd.DataFrame(spam_test_bern).to_numpy()\n",
        "\n",
        "  train_bern_arr = np.append(ham_arr, spam_arr, axis=0)\n",
        "  test_bern_arr = np.append(test_ham_bern_arr, test_spam_bern_arr, axis=0)\n",
        "\n",
        "  return ham_arr, spam_arr, train_bern_arr, test_bern_arr, Voc_dict"
      ],
      "metadata": {
        "id": "3QTsDWzYmVkq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnb_train(ham_bow_arr, spam_bow_arr, size_of_ham, size_of_spam, size_of_total, Voc_dict):\n",
        "  '''\n",
        "  This function takes the bag of words representation of the data, and also the vocabulary dictionary \n",
        "  and returns the priors and conditional probability for the Multinomial NB\n",
        "  '''\n",
        "  prior = {}\n",
        "\n",
        "  prior['ham'] = math.log(size_of_ham/float(size_of_total))\n",
        "  prior['spam'] = math.log(size_of_spam/float(size_of_total))\n",
        "\n",
        "  # summation of all the columns\n",
        "  ham_bow_column_sum = np.sum(ham_bow_arr, axis=0)\n",
        "  spam_bow_column_sum = np.sum(spam_bow_arr, axis=0)\n",
        "\n",
        "  # total number of words\n",
        "  ham_bow_total = np.sum(ham_bow_column_sum)\n",
        "  spam_bow_total = np.sum(spam_bow_column_sum)\n",
        "\n",
        "  # calculating conditional prob and storing them in log\n",
        "  ham_cond_prob = np.log((ham_bow_column_sum+1)/(ham_bow_total+len(Voc_dict)))\n",
        "  spam_cond_prob = np.log((spam_bow_column_sum+1)/(spam_bow_total+len(Voc_dict)))\n",
        "\n",
        "  return prior, ham_cond_prob, spam_cond_prob"
      ],
      "metadata": {
        "id": "cOdajk3DVYWI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnb_test(prior, ham_cond_prob, spam_cond_prob, test_bow_arr, Y_test):\n",
        "  '''\n",
        "  This function takes the priors and conditional probability, along with the test data for the Multinomial NB\n",
        "  and returns the necessary metrics such as accuracy\n",
        "  '''\n",
        "\n",
        "  # multiplying test_bow_arr with log conditional probability \n",
        "  # and then taking the sum over all features along with their respective prior\n",
        "  # and finally exponenting the result\n",
        "  Y_predict_ham = np.exp(np.sum(np.multiply(test_bow_arr, ham_cond_prob), axis=1) + (prior['ham']))\n",
        "  Y_predict_spam = np.exp(np.sum(np.multiply(test_bow_arr, spam_cond_prob), axis=1) + (prior['spam']))\n",
        "\n",
        "  Y_bow_predict = []\n",
        "  for i,j in zip(Y_predict_ham, Y_predict_spam):\n",
        "    if i>j:\n",
        "      Y_bow_predict.append(1)\n",
        "    else:\n",
        "      Y_bow_predict.append(0)\n",
        "\n",
        "  # calculating the necessary metrics\n",
        "  accuracy_mnb = accuracy_score(Y_test, Y_bow_predict)\n",
        "  scores_mnb = precision_recall_fscore_support(Y_test, Y_bow_predict, average=\"macro\")\n",
        "\n",
        "  return accuracy_mnb, scores_mnb"
      ],
      "metadata": {
        "id": "K3fMXFcsWi_g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnb(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number \n",
        "  and returns accuracy and other metrics after training and testing the Multinomial NB\n",
        "  '''\n",
        "\n",
        "  # Get training and testing dataset from the dataset number\n",
        "  files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam = get_data(dataset_num)\n",
        "\n",
        "  # Get the Training and Testing Bag of Words \n",
        "  ham_train_arr, spam_train_arr, train_bow_arr, test_bow_arr, Voc_dict = getting_bow(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam)\n",
        "\n",
        "  # Implement or Train the Multinoial NB and get the priors\n",
        "  prior, ham_cond_prob, spam_cond_prob = mnb_train(ham_train_arr, spam_train_arr, size_of_ham, size_of_spam, size_of_total, Voc_dict)\n",
        "\n",
        "  # array of output of testing data\n",
        "  Y_test = []\n",
        "  for i in range(len(test_files_ham)):\n",
        "    Y_test.append(1)\n",
        "  for i in range(len(test_files_spam)):\n",
        "    Y_test.append(0)  \n",
        "\n",
        "  # Testing the MNB implementation\n",
        "  accuracy_mnb, scores_mnb = mnb_test(prior, ham_cond_prob, spam_cond_prob, test_bow_arr, Y_test)\n",
        "\n",
        "  return accuracy_mnb, scores_mnb\n"
      ],
      "metadata": {
        "id": "sMh7l95OHiKN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dnb_train(ham_bern_arr, spam_bern_arr, size_of_ham, size_of_spam, size_of_total, Voc_dict):\n",
        "  '''\n",
        "  This function takes the bernoulli representation of the data, and also the vocabulary dictionary \n",
        "  and returns the priors and conditional probability for the Discrete NB\n",
        "  '''\n",
        "  prior = {}\n",
        "\n",
        "  prior['ham'] = math.log(size_of_ham/float(size_of_total))\n",
        "  prior['spam'] = math.log(size_of_spam/float(size_of_total))\n",
        "\n",
        "  # summation of all the columns\n",
        "  ham_bern_column_sum = np.sum(ham_bern_arr, axis=0)\n",
        "  spam_bern_column_sum = np.sum(spam_bern_arr, axis=0)\n",
        "\n",
        "  # total number of words\n",
        "  ham_bern_total = np.sum(ham_bern_column_sum)\n",
        "  spam_bern_total = np.sum(spam_bern_column_sum)\n",
        "\n",
        "  # calculating conditional prob and storing them in log\n",
        "  ham_cond_prob = np.log((np.count_nonzero(ham_bern_arr, axis=0)+1)/(size_of_ham+2))\n",
        "  spam_cond_prob = np.log((np.count_nonzero(spam_bern_arr, axis=0)+1)/(size_of_spam+2))\n",
        "\n",
        "  return prior, ham_cond_prob, spam_cond_prob"
      ],
      "metadata": {
        "id": "wnuU6spenTnH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dnb_test(prior, ham_cond_prob, spam_cond_prob, test_bern_arr, Y_test):\n",
        "  '''\n",
        "  This function takes the priors and conditional probability, along with the test data for the Discrete NB\n",
        "  and returns the necessary metrics such as accuracy\n",
        "  '''\n",
        "  \n",
        "  # multiplying test_bern_arr with log conditional probability \n",
        "  # and then taking the sum over all features along with their respective prior\n",
        "  # and finally exponenting the result\n",
        "\n",
        "  Y_predict_ham = np.exp(np.sum(np.multiply(test_bern_arr, ham_cond_prob), axis=1) + (prior['ham']))\n",
        "  Y_predict_spam = np.exp(np.sum(np.multiply(test_bern_arr, spam_cond_prob), axis=1) + (prior['spam']))\n",
        "\n",
        "  Y_bern_predict = []\n",
        "  for i,j in zip(Y_predict_ham, Y_predict_spam):\n",
        "    if i>j:\n",
        "      Y_bern_predict.append(1)\n",
        "    else:\n",
        "      Y_bern_predict.append(0)\n",
        "\n",
        "  # calculating the necessary metrics\n",
        "  accuracy_dnb = accuracy_score(Y_test, Y_bern_predict)\n",
        "  scores_dnb = precision_recall_fscore_support(Y_test, Y_bern_predict, average=\"macro\")\n",
        "\n",
        "  return accuracy_dnb, scores_dnb"
      ],
      "metadata": {
        "id": "IAyzlPutnnjq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dnb(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number \n",
        "  and returns accuracy and other metrics after training and testing the Discrete NB\n",
        "  '''\n",
        "\n",
        "  # Get training and testing dataset from the dataset number\n",
        "  files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam = get_data(dataset_num)\n",
        "\n",
        "  # Get the Training and Testing Bernoulli model \n",
        "  ham_train_arr, spam_train_arr, train_bern_arr, test_bern_arr, Voc_dict = getting_bern(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam)\n",
        "\n",
        "  # Implement or Train the Discrete NB and get the priors\n",
        "  prior, ham_cond_prob, spam_cond_prob = dnb_train(ham_train_arr, spam_train_arr, size_of_ham, size_of_spam, size_of_total, Voc_dict)\n",
        "\n",
        "  # array of output of testing data\n",
        "  Y_test = []\n",
        "  for i in range(len(test_files_ham)):\n",
        "    Y_test.append(1)\n",
        "  for i in range(len(test_files_spam)):\n",
        "    Y_test.append(0)  \n",
        "\n",
        "  # Testing the DNB implementation\n",
        "  accuracy_dnb, scores_dnb = dnb_test(prior, ham_cond_prob, spam_cond_prob, test_bern_arr, Y_test)\n",
        "\n",
        "  return accuracy_dnb, scores_dnb"
      ],
      "metadata": {
        "id": "wznBrT7PlMuu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_predict(x, w0, w):\n",
        "  '''\n",
        "  This function takes the weights and data\n",
        "  and returns the sigmoid of the summation\n",
        "  '''\n",
        "  summation = w0 + np.sum(np.multiply(w, x), axis=1)\n",
        "  pred_y = np.where(\n",
        "            summation >= 0, # condition\n",
        "            1 / (1 + np.exp(-summation)),\n",
        "            np.exp(summation) / (1 + np.exp(summation))\n",
        "            )\n",
        "  return pred_y"
      ],
      "metadata": {
        "id": "Ri-psSPjxRD2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_train(x_train, y_train, lam, S):\n",
        "  '''\n",
        "  This function takes the training data, along with the lambda and maximum number of iterations\n",
        "  and returns the weights after training \n",
        "  '''\n",
        "  W0 = 0.0\n",
        "  W = np.zeros(len(x_train[0]))  \n",
        "  L = 0.01     \n",
        "  CLL_log = []\n",
        "\n",
        "  for epoch in range(S):\n",
        "    y_predict_lr = lr_predict(x_train, W0, W)\n",
        "    cll_w0 = np.sum(np.subtract(y_train, y_predict_lr))\n",
        "    cll = np.transpose(x_train)@np.subtract(y_train, y_predict_lr)\n",
        "\n",
        "    # update the weights, according to gradient ascent and L2 regularization\n",
        "    W0 = W0 + L * (cll_w0 - (lam*W0))\n",
        "    W = np.add(W, (np.multiply(L, (np.subtract(cll, np.multiply(lam, W))))))\n",
        "\n",
        "    log_cll = np.sum(cll)/len(cll)\n",
        "    CLL_log.append(log_cll)\n",
        "\n",
        "    # Since it is a small dataset the convergence can be achieved very quickly\n",
        "    if log_cll<0.1:\n",
        "      break\n",
        "\n",
        "  return W0, W, CLL_log"
      ],
      "metadata": {
        "id": "rMdAcwoctiEs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_test(x, W0, W):\n",
        "  '''\n",
        "  This function takes the weights and data\n",
        "  and returns the prediction of the test data using the sigmoid function lr_predict\n",
        "  '''\n",
        "  pred_y = lr_predict(x, W0, W)\n",
        "  y_prediction = []\n",
        "  for pred in pred_y:\n",
        "    if pred>0.5:\n",
        "      y_prediction.append(1)\n",
        "    else:\n",
        "      y_prediction.append(0)\n",
        "  return y_prediction\n",
        "\n",
        "def lr_validate(x_train, y_train, x_vali, y_vali):\n",
        "  '''\n",
        "  This function takes the training and validation data\n",
        "  and returns the Best Lambda by training and validating on the respective data\n",
        "  '''\n",
        "  L = 0.01\n",
        "  best_lambda = 0.2\n",
        "  best_acc = 0\n",
        "  for lam in [0.1, 0.3, 0.5]:\n",
        "    w0, w, cll_log = lr_train(x_train, y_train, lam, 30)\n",
        "    y_predict_mcap = lr_test(x_vali, w0, w)\n",
        "    acc = accuracy_score(y_vali, y_predict_mcap)\n",
        "    if acc > best_acc:\n",
        "      best_acc = acc\n",
        "      best_lambda = lam\n",
        "  return best_lambda"
      ],
      "metadata": {
        "id": "gGzCVHh-zLLv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcap_bow(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number\n",
        "  and returns accuracy and other metrics after training and testing \n",
        "  the LR model for Bag of Words representation\n",
        "  '''\n",
        "  # Get training and testing dataset from the dataset number\n",
        "  files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam = get_data(dataset_num)\n",
        "\n",
        "  # Get the Training and Testing Bag of Words \n",
        "  ham_train_arr, spam_train_arr, train_bow_arr, test_bow_arr, Voc_dict = getting_bow(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam)\n",
        "\n",
        "  # array of output of training data\n",
        "  Y_train_arr = []\n",
        "  for i in range(len(files_ham)):\n",
        "    Y_train_arr.append(1)\n",
        "  for i in range(len(files_spam)):\n",
        "    Y_train_arr.append(0)\n",
        "\n",
        "  # array of output of testing data\n",
        "  Y_test = []\n",
        "  for i in range(len(test_files_ham)):\n",
        "    Y_test.append(1)\n",
        "  for i in range(len(test_files_spam)):\n",
        "    Y_test.append(0) \n",
        "\n",
        "  # create train and validation sets\n",
        "  x_train, x_vali, y_train, y_vali = train_test_split(train_bow_arr, Y_train_arr, test_size=0.3, random_state=5)\n",
        "\n",
        "  # We will now find the best lambda by training on train data and validating on validation data over different lambda values\n",
        "  Best_lambda = lr_validate(x_train, y_train, x_vali, y_vali)\n",
        "\n",
        "  # Now train the model using the best lambda and full training data\n",
        "  W0_final, W_final, CLL_log_final = lr_train(train_bow_arr, Y_train_arr, Best_lambda, 80)\n",
        "\n",
        "  # Testing the MCAP LR BOW implementation\n",
        "  Y_mcap_bow_predict = lr_test(test_bow_arr, W0_final, W_final)\n",
        "\n",
        "  # calculating the necessary metrics\n",
        "  accuracy_mcap = accuracy_score(Y_test, Y_mcap_bow_predict)\n",
        "  scores_mcap = precision_recall_fscore_support(Y_test, Y_mcap_bow_predict, average=\"macro\")\n",
        "\n",
        "  return accuracy_mcap, scores_mcap"
      ],
      "metadata": {
        "id": "KWutvxooozBu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcap_bern(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number\n",
        "  and returns accuracy and other metrics after training and testing \n",
        "  the LR model for Bernoulli representation\n",
        "  '''\n",
        "  # Get training and testing dataset from the dataset number\n",
        "  files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam = get_data(dataset_num)\n",
        "\n",
        "  # Get the Training and Testing Bernoulli Model\n",
        "  ham_train_arr, spam_train_arr, train_bern_arr, test_bern_arr, Voc_dict = getting_bern(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam)\n",
        "\n",
        "  # array of output of training data\n",
        "  Y_train_arr = []\n",
        "  for i in range(len(files_ham)):\n",
        "    Y_train_arr.append(1)\n",
        "  for i in range(len(files_spam)):\n",
        "    Y_train_arr.append(0)\n",
        "\n",
        "  # array of output of testing data\n",
        "  Y_test = []\n",
        "  for i in range(len(test_files_ham)):\n",
        "    Y_test.append(1)\n",
        "  for i in range(len(test_files_spam)):\n",
        "    Y_test.append(0) \n",
        "\n",
        "  # create train and validation sets\n",
        "  x_train, x_vali, y_train, y_vali = train_test_split(train_bern_arr, Y_train_arr, test_size=0.3, random_state=5)\n",
        "\n",
        "  # We will now find the best lambda by training on train data and validating on validation data over different lambda values\n",
        "  Best_lambda = lr_validate(x_train, y_train, x_vali, y_vali)\n",
        "\n",
        "  # Now train the model using the best lambda and full training data\n",
        "  W0_final, W_final, CLL_log_final = lr_train(train_bern_arr, Y_train_arr, Best_lambda, 80)\n",
        "\n",
        "  # Testing the MCAP LR Bernoulli implementation\n",
        "  Y_mcap_bern_predict = lr_test(test_bern_arr, W0_final, W_final)\n",
        "\n",
        "  # calculating the necessary metrics\n",
        "  accuracy_mcap = accuracy_score(Y_test, Y_mcap_bern_predict)\n",
        "  scores_mcap = precision_recall_fscore_support(Y_test, Y_mcap_bern_predict, average=\"macro\")\n",
        "\n",
        "  return accuracy_mcap, scores_mcap"
      ],
      "metadata": {
        "id": "TwtB5fT9U6LF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sgd_bow(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number\n",
        "  and returns accuracy and other metrics after training and testing \n",
        "  the SGDClassifier model for the Bag of Words representation\n",
        "  '''\n",
        "  # Get training and testing dataset from the dataset number\n",
        "  files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam = get_data(dataset_num)\n",
        "\n",
        "  # Get the Training and Testing Bag of Words \n",
        "  ham_train_arr, spam_train_arr, train_bow_arr, test_bow_arr, Voc_dict = getting_bow(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam)\n",
        "\n",
        "  # array of output of training data\n",
        "  Y_train_arr = []\n",
        "  for i in range(len(files_ham)):\n",
        "    Y_train_arr.append(1)\n",
        "  for i in range(len(files_spam)):\n",
        "    Y_train_arr.append(0)\n",
        "\n",
        "  # array of output of testing data\n",
        "  Y_test = []\n",
        "  for i in range(len(test_files_ham)):\n",
        "    Y_test.append(1)\n",
        "  for i in range(len(test_files_spam)):\n",
        "    Y_test.append(0) \n",
        "\n",
        "  # create train and validation sets\n",
        "  x_train, x_vali, y_train, y_vali = train_test_split(train_bow_arr, Y_train_arr, test_size=0.3, random_state=5)\n",
        "\n",
        "  model = SGDClassifier()\n",
        "  # Tuning the paramters using GridSearchCV\n",
        "  parameters = {'alpha': (0.001, 0.01), 'loss': ['squared_hinge', 'hinge', 'log'], 'max_iter': [30, 50, 80]}\n",
        "  grid_search = GridSearchCV(model, param_grid=parameters)    \n",
        "  classifier = grid_search.fit(x_vali, y_vali)\n",
        "\n",
        "  # Now we train the model on the basis of the tuned parameters\n",
        "  cls = classifier.fit(x_train, y_train)\n",
        "\n",
        "  # Testing the implementation of SGD BOW\n",
        "  Y_sgd_bow_predict = cls.predict(test_bow_arr)\n",
        "  \n",
        "  # calculating the necessary metrics\n",
        "  accuracy_sgd = accuracy_score(Y_test, Y_sgd_bow_predict)\n",
        "  scores_sgd = precision_recall_fscore_support(Y_test, Y_sgd_bow_predict, average=\"macro\")\n",
        "\n",
        "  return accuracy_sgd, scores_sgd"
      ],
      "metadata": {
        "id": "x7hM8-_bKxu7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_bern(dataset_num):\n",
        "  '''\n",
        "  This function takes the dataset number\n",
        "  and returns accuracy and other metrics after training and testing \n",
        "  the SGDClassifier model for the Bernoulli representation\n",
        "  '''\n",
        "  # Get training and testing dataset from the dataset number\n",
        "  files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam = get_data(dataset_num)\n",
        "\n",
        "  # Get the Training and Testing Bernoulli\n",
        "  ham_train_arr, spam_train_arr, train_bern_arr, test_bern_arr, Voc_dict = getting_bern(files_ham, files_spam, size_of_ham, size_of_spam, size_of_total, test_files_ham, test_files_spam)\n",
        "\n",
        "  # array of output of training data\n",
        "  Y_train_arr = []\n",
        "  for i in range(len(files_ham)):\n",
        "    Y_train_arr.append(1)\n",
        "  for i in range(len(files_spam)):\n",
        "    Y_train_arr.append(0)\n",
        "\n",
        "  # array of output of testing data\n",
        "  Y_test = []\n",
        "  for i in range(len(test_files_ham)):\n",
        "    Y_test.append(1)\n",
        "  for i in range(len(test_files_spam)):\n",
        "    Y_test.append(0) \n",
        "\n",
        "  # create train and validation sets\n",
        "  x_train, x_vali, y_train, y_vali = train_test_split(train_bern_arr, Y_train_arr, test_size=0.3, random_state=5)\n",
        "\n",
        "  model = SGDClassifier()\n",
        "  # Tuning the paramters using GridSearchCV\n",
        "  parameters = {'alpha': (0.001, 0.01), 'loss': ['squared_hinge', 'hinge', 'log'], 'max_iter': [30, 50, 80]}\n",
        "  grid_search = GridSearchCV(model, param_grid=parameters)  \n",
        "  classifier = grid_search.fit(x_vali, y_vali)\n",
        "\n",
        "  # Now we train the model on the basis of the tuned parameters\n",
        "  cls = classifier.fit(x_train, y_train)\n",
        "\n",
        "  # Testing the implementation of SGD BOW\n",
        "  Y_sgd_bern_predict = cls.predict(test_bern_arr)\n",
        "  \n",
        "  # calculating the necessary metrics\n",
        "  accuracy_sgd = accuracy_score(Y_test, Y_sgd_bern_predict)\n",
        "  scores_sgd = precision_recall_fscore_support(Y_test, Y_sgd_bern_predict, average=\"macro\")\n",
        "\n",
        "  return accuracy_sgd, scores_sgd"
      ],
      "metadata": {
        "id": "3OzRD_fZH0ml"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting input from user for dataset name, algorithm name, and type of representation\n",
        "'''\n",
        "Enter the dataset number: This can take the numerical values such as '1', '2', and '4' which coreesponds \n",
        "\tto the datasets named - enron1, enron2, enron4\n",
        "Enter the algorithm name: This can take the following values:\n",
        "\t'mnb' for Multinomial Naive Bayes\n",
        "\t'dnb' for Discrete Naive Bayes\n",
        "\t'mcap' for MCAP Logistic Regression\n",
        "\t'sgd' for SGDClassifier\n",
        "Enter the representation: This can take only 2 values:\n",
        "\t'bow' for Bag of Words representation\n",
        "\t'bern' for Bernoulli representation\n",
        "Kindly input the 'bow' for MNB and 'bern' for DNB\n",
        "'''\n",
        "\n",
        "dataset_number = input('Enter the dataset number: ')\n",
        "algo_name = input('Enter the algorithm name: ')\n",
        "rep_type = input('Enter the type of representation: ')\n",
        "\n",
        "if algo_name == 'mnb':\n",
        "  rep_type = 'bow'\n",
        "  Acc, Scores = mnb(dataset_number)\n",
        "\n",
        "elif algo_name == 'dnb':\n",
        "  rep_type = 'bern'\n",
        "  Acc, Scores = dnb(dataset_number)\n",
        "\n",
        "elif algo_name == 'mcap':\n",
        "  if rep_type == 'bow':\n",
        "    Acc, Scores = mcap_bow(dataset_number)\n",
        "  elif rep_type == 'bern':\n",
        "    Acc, Scores = mcap_bern(dataset_number)\n",
        "\n",
        "elif algo_name == 'sgd':\n",
        "  if rep_type == 'bow':\n",
        "    Acc, Scores = sgd_bow(dataset_number)\n",
        "  elif rep_type == 'bern':\n",
        "    Acc, Scores = sgd_bern(dataset_number)\n",
        "\n",
        "print('Following evaluations are for {} algorithm and {} representation type'.format(algo_name, rep_type), '\\n', 'Model Accuracy :', Acc, '\\n', 'Model Precision :', Scores[0], '\\n', 'Model Recall :', Scores[1], '\\n', 'Model F1 Score : ', Scores[2])"
      ],
      "metadata": {
        "id": "KYC5VcPAEePr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run this only if you want to run and get metrics for all Algorithms with each of the 3 datasets and 2 representation type\n",
        "\n",
        "def show(Acc, Scores, algo_name, rep_type, i):\n",
        "  '''\n",
        "  This function takes accuracy and other metrics, along with the algorithm name and representation type\n",
        "  and dataset name\n",
        "  and outputs them in a neat fashion\n",
        "  '''\n",
        "  print('Following evaluations are for {} algorithm and {} representation type with dataset {}'.format(algo_name, rep_type, i), '\\n', 'Model Accuracy :', Acc, '\\n', 'Model Precision :', Scores[0], '\\n', 'Model Recall :', Scores[1], '\\n', 'Model F1 Score : ', Scores[2])\n",
        "\n",
        "for algo_name in ['mnb', 'dnb', 'mcap', 'sgd']:\n",
        "  if algo_name == 'mnb':\n",
        "    rep_type = 'bow'\n",
        "    for i in [1,2,4]:\n",
        "      Acc, Scores = mnb(i)\n",
        "      show(Acc, Scores, algo_name, rep_type, i)\n",
        "\n",
        "  elif algo_name == 'dnb':\n",
        "    rep_type = 'bern'\n",
        "    for i in [1,2,4]:\n",
        "      Acc, Scores = dnb(i)\n",
        "      show(Acc, Scores, algo_name, rep_type, i)\n",
        "\n",
        "  elif algo_name == 'mcap':\n",
        "    rep_type = 'bow'\n",
        "    for i in [1,2,4]:\n",
        "      Acc, Scores = mcap_bow(i)\n",
        "      show(Acc, Scores, algo_name, rep_type, i)\n",
        "    rep_type = 'bern'\n",
        "    for i in [1,2,4]:\n",
        "      Acc, Scores = mcap_bern(i)\n",
        "      show(Acc, Scores, algo_name, rep_type, i)\n",
        "\n",
        "  elif algo_name == 'sgd':\n",
        "    rep_type = 'bow'\n",
        "    for i in [1,2,4]:\n",
        "      Acc, Scores = sgd_bow(i)\n",
        "      show(Acc, Scores, algo_name, rep_type, i)\n",
        "    rep_type = 'bern'\n",
        "    for i in [1,2,4]:\n",
        "      Acc, Scores = sgd_bern(i)\n",
        "      show(Acc, Scores, algo_name, rep_type, i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53b0746-f1e2-4a45-c510-2ab360fd5a62",
        "id": "hRBS1X0Aww7h"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Following evaluations are for mnb algorithm and bow representation type with dataset 1 \n",
            " Model Accuracy : 0.7280701754385965 \n",
            " Model Precision : 0.7608108906794855 \n",
            " Model Recall : 0.7894104015915004 \n",
            " Model F1 Score :  0.7255145631067961\n",
            "Following evaluations are for mnb algorithm and bow representation type with dataset 2 \n",
            " Model Accuracy : 0.7364016736401674 \n",
            " Model Precision : 0.749543475207192 \n",
            " Model Recall : 0.8141467727674625 \n",
            " Model F1 Score :  0.725322436470438\n",
            "Following evaluations are for mnb algorithm and bow representation type with dataset 4 \n",
            " Model Accuracy : 0.8876611418047882 \n",
            " Model Precision : 0.9325221238938053 \n",
            " Model Recall : 0.799342105263158 \n",
            " Model F1 Score :  0.8383052882855175\n",
            "Following evaluations are for dnb algorithm and bern representation type with dataset 1 \n",
            " Model Accuracy : 0.8377192982456141 \n",
            " Model Precision : 0.8820698342969734 \n",
            " Model Recall : 0.7585860131604836 \n",
            " Model F1 Score :  0.7861054766734281\n",
            "Following evaluations are for dnb algorithm and bern representation type with dataset 2 \n",
            " Model Accuracy : 0.8723849372384938 \n",
            " Model Precision : 0.8981789917832557 \n",
            " Model Recall : 0.7774314765694076 \n",
            " Model F1 Score :  0.8132369990328139\n",
            "Following evaluations are for dnb algorithm and bern representation type with dataset 4 \n",
            " Model Accuracy : 0.996316758747698 \n",
            " Model Precision : 0.9974554707379135 \n",
            " Model Recall : 0.993421052631579 \n",
            " Model F1 Score :  0.9954132315177726\n",
            "Following evaluations are for mcap algorithm and bow representation type with dataset 1 \n",
            " Model Accuracy : 0.9429824561403509 \n",
            " Model Precision : 0.9277725710715401 \n",
            " Model Recall : 0.9490195221126729 \n",
            " Model F1 Score :  0.9368595956798671\n",
            "Following evaluations are for mcap algorithm and bow representation type with dataset 2 \n",
            " Model Accuracy : 0.9707112970711297 \n",
            " Model Precision : 0.9541842772612004 \n",
            " Model Recall : 0.9750663129973476 \n",
            " Model F1 Score :  0.9638699924414211\n",
            "Following evaluations are for mcap algorithm and bow representation type with dataset 4 \n",
            " Model Accuracy : 0.8802946593001841 \n",
            " Model Precision : 0.9287280701754386 \n",
            " Model Recall : 0.7861842105263157 \n",
            " Model F1 Score :  0.8256460162127717\n",
            "Following evaluations are for mcap algorithm and bern representation type with dataset 1 \n",
            " Model Accuracy : 1.0 \n",
            " Model Precision : 1.0 \n",
            " Model Recall : 1.0 \n",
            " Model F1 Score :  1.0\n",
            "Following evaluations are for mcap algorithm and bern representation type with dataset 2 \n",
            " Model Accuracy : 0.99581589958159 \n",
            " Model Precision : 0.9971428571428571 \n",
            " Model Recall : 0.9923076923076923 \n",
            " Model F1 Score :  0.994691366251305\n",
            "Following evaluations are for mcap algorithm and bern representation type with dataset 4 \n",
            " Model Accuracy : 0.994475138121547 \n",
            " Model Precision : 0.9961928934010152 \n",
            " Model Recall : 0.9901315789473684 \n",
            " Model F1 Score :  0.9931057832702034\n",
            "Following evaluations are for sgd algorithm and bow representation type with dataset 1 \n",
            " Model Accuracy : 0.9868421052631579 \n",
            " Model Precision : 0.9885131389289279 \n",
            " Model Recall : 0.9815928120149531 \n",
            " Model F1 Score :  0.9849415494353081\n",
            "Following evaluations are for sgd algorithm and bow representation type with dataset 2 \n",
            " Model Accuracy : 0.99581589958159 \n",
            " Model Precision : 0.9924242424242424 \n",
            " Model Recall : 0.9971264367816092 \n",
            " Model F1 Score :  0.9947422839166684\n",
            "Following evaluations are for sgd algorithm and bow representation type with dataset 4 \n",
            " Model Accuracy : 1.0 \n",
            " Model Precision : 1.0 \n",
            " Model Recall : 1.0 \n",
            " Model F1 Score :  1.0\n",
            "Following evaluations are for sgd algorithm and bern representation type with dataset 1 \n",
            " Model Accuracy : 1.0 \n",
            " Model Precision : 1.0 \n",
            " Model Recall : 1.0 \n",
            " Model F1 Score :  1.0\n",
            "Following evaluations are for sgd algorithm and bern representation type with dataset 2 \n",
            " Model Accuracy : 1.0 \n",
            " Model Precision : 1.0 \n",
            " Model Recall : 1.0 \n",
            " Model F1 Score :  1.0\n",
            "Following evaluations are for sgd algorithm and bern representation type with dataset 4 \n",
            " Model Accuracy : 0.992633517495396 \n",
            " Model Precision : 0.9949367088607595 \n",
            " Model Recall : 0.986842105263158 \n",
            " Model F1 Score :  0.9907888040712467\n"
          ]
        }
      ]
    }
  ]
}